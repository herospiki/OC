{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import arange, argmin, argmax\n",
    "import pandas_flavor as pf\n",
    "import tqdm\n",
    "import re\n",
    "from random import random\n",
    "from time import time\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data_to_train.pkl')\n",
    "data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pf.register_dataframe_method\n",
    "def add_row(df, row):\n",
    "    df.loc[len(df)] = row\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de stockage des résultats dans des tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_tables():\n",
    "    table_scores = PrettyTable()\n",
    "    scores_df = pd.DataFrame([], columns=[\"model name\", \"step\", \"time\",\n",
    "                             \"roc AUC score\", \"accuracy\", \"F2-score\", \"precision\", \"recall\"])\n",
    "    table_scores.field_names = [\"model name\", \"step\", \"time\",\n",
    "                                \"roc AUC score\", \"accuracy\",  \"F2-score\", \"precision\", \"recall\"]\n",
    "    return scores_df, table_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_for_testing(data,ratio):\n",
    "    data_0 = data[data.TARGET == 0]\n",
    "    data_1 = data[data.TARGET == 1]\n",
    "    data_0 = data_0.sample(int(round(len(data_0)*ratio, 0)))\n",
    "    data_1 = data_1.sample(int(round(len(data_1)*ratio, 0)))\n",
    "    data = data_1.append(data_0)\n",
    "    del data_0, data_1\n",
    "    gc.collect()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " sklearn.metrics.fbeta_score(y_true, y_pred, *, beta, labels=None, pos_label=1, average='binary', sample_weight=None, \n",
    " zero_division='warn')[source]\n",
    "\n",
    "    Compute the F-beta score.\n",
    "\n",
    "    The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and \n",
    "    its worst value at 0.\n",
    "\n",
    "    The beta parameter determines the weight of recall in the combined score. beta < 1 lends more weight to precision, \n",
    "    while beta > 1 favors recall (beta -> 0 considers only precision, beta -> +inf only recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation et optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_and_log(model_name, model_pipeline,step, time, x_test, y_test, scores_df, table_scores):\n",
    "    test_pred = model_pipeline.predict(x_test)\n",
    "    test_pred_proba = model_pipeline.predict_proba(x_test)\n",
    "\n",
    "    auc_score = roc_auc_score(y_test, test_pred_proba[:, 1])\n",
    "    accuracy = accuracy_score(y_test, test_pred)\n",
    "    F2_score = fbeta_score(y_test, test_pred, beta=2)\n",
    "    precision = precision_score(y_test, test_pred)\n",
    "    recall = recall_score(y_test, test_pred)\n",
    "\n",
    "    scores_df.add_row([model_name, step, time, auc_score,accuracy, F2_score, precision, recall])\n",
    "    table_scores.add_row([model_name, step, time, auc_score,accuracy, F2_score, precision, recall])\n",
    "    #print('Confusion matrix:\\n', confusion_matrix(y_test, test_pred))\n",
    "    return scores_df, table_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_pipeline, x_test, y_test):\n",
    "    # prediction\n",
    "    test_pred = model_pipeline.predict(x_test)\n",
    "    test_pred_proba = model_pipeline.predict_proba(x_test)\n",
    "    print('Roc auc score : {:.4f}'.format(\n",
    "        roc_auc_score(y_test, test_pred_proba[:, 1])))\n",
    "    print('F2-score : {:.4f}'.format(fbeta_score(y_test, test_pred, beta=2)))\n",
    "    print('Accuracy :{:.4f}'.format(accuracy_score(y_test, test_pred)))\n",
    "    print('Precision :{:.4f}'.format(precision_score(y_test, test_pred)))\n",
    "    print('Recall : {:.4f}'.format( recall_score(y_test, test_pred)))\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    " return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "def evaluate_model_with_threshold(model_pipeline,x_test, y_test, threshold):\n",
    "    # prediction\n",
    "\n",
    "    test_pred_proba = model_pipeline.predict_proba(x_test)\n",
    "    test_pred_th = to_labels(test_pred_proba, threshold)[::,1]\n",
    "    print('Roc auc score : {:.4f}'.format(roc_auc_score(y_test, test_pred_th)))\n",
    "    print('F2-score : {:.4f}'.format(fbeta_score(y_test, test_pred_th, beta=2)))\n",
    "    print('Accuracy :{:.4f}'.format(accuracy_score(y_test, test_pred_th)))\n",
    "    print('Precision :{:.4f}'.format(precision_score(y_test, test_pred_th)))\n",
    "    print('Recall : {:.4f}'.format( recall_score(y_test, test_pred_th)))\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_test, test_pred_th))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scorer utilisé : fbeta_score avec beta = 2 pour donner plus de poids à la classe positive qui est minoritaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests de plusieurs algorithmes sur un sous-ensemble des données "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On prend la moitié des données uniquement pour accélérer l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = get_sample_for_testing(data, 0.5)\n",
    "y_sample = data_sample[['TARGET']]\n",
    "X_sample = data_sample.drop(columns=['SK_ID_CURR','TARGET'])\n",
    "features = X_sample.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_train, X_sample_valid, y_sample_train, y_sample_valid = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.20, random_state=42, stratify=y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results for Dummy Classification \n",
      "-------------------------------\n",
      "Roc auc score : 0.4976\n",
      "F2-score : 0.0812\n",
      "Accuracy :0.8512\n",
      "Precision :0.0808\n",
      "Recall : 0.0813\n",
      "Confusion matrix:\n",
      " [[103891   9183]\n",
      " [  9123    807]]\n",
      "===============================\n",
      "Validation Results for Dummy Classification \n",
      "-------------------------------\n",
      "Roc auc score : 0.4978\n",
      "F2-score : 0.0879\n",
      "Accuracy :0.8512\n",
      "Precision :0.0865\n",
      "Recall : 0.0882\n",
      "Confusion matrix:\n",
      " [[25956  2313]\n",
      " [ 2263   219]]\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "dummy_clf.fit(X_sample_train,y_sample_train)\n",
    "y_dummy_pred = dummy_clf.predict(X_sample_valid)\n",
    "\n",
    "print('Training Results for Dummy Classification ')\n",
    "print('-------------------------------')\n",
    "evaluate_model(dummy_clf,X_sample_train, y_sample_train)\n",
    "print('===============================')\n",
    "print('Validation Results for Dummy Classification ')\n",
    "print('-------------------------------')\n",
    "evaluate_model(dummy_clf,X_sample_valid, y_sample_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement et évaluation plusieurs classifieurs binaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour comparer quelques modèles, sans optimisation, pour choisir le meilleur modèle à optimiser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X, y, with_smote=False, worf = False):\n",
    "    scores_df, table_scores = init_tables()\n",
    "    scale_pos_weight = Counter(y['TARGET'])[0]/Counter(y['TARGET'])[1]\n",
    "    \n",
    "    classifiers = [\n",
    "            ('Logistic Regression', LogisticRegression(class_weight='balanced')),\n",
    "            ('RandomForest', RandomForestClassifier(class_weight='balanced')),\n",
    "            ('XGBoost', XGBClassifier(scale_pos_weight=scale_pos_weight)),\n",
    "            ('Light GBM', LGBMClassifier(objective='binary', scale_pos_weight=scale_pos_weight))\n",
    "            ]\n",
    "    if (worf == True) : \n",
    "        classifiers = [\n",
    "            ('Logistic Regression', LogisticRegression()),\n",
    "            ('XGBoost', XGBClassifier()),\n",
    "            ('Light GBM', LGBMClassifier(objective='binary'))\n",
    "        ]\n",
    "\n",
    "    skfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=100)\n",
    "\n",
    "    for clf_name, clf in tqdm.tqdm(classifiers):\n",
    "        print(clf_name)\n",
    "        print('===============================')\n",
    "        # Entraîner le classifieur sur les données d'entraînement\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('scaler', RobustScaler()),\n",
    "            ('classifier', clf)\n",
    "        ]\n",
    "        )\n",
    "\n",
    "        for i, (train_index, test_index) in enumerate(skfolds.split(X, y)):\n",
    "            start = time()\n",
    "            X_train = X.iloc[train_index]\n",
    "            y_train = y.iloc[train_index]\n",
    "            X_test = X.iloc[test_index]\n",
    "            y_test = y.iloc[test_index]\n",
    "            if with_smote:\n",
    "                over_only = SMOTE()\n",
    "                print('Before sampling')\n",
    "                print(Counter(y_train['TARGET']))\n",
    "\n",
    "                # transform the dataset\n",
    "\n",
    "                X_train_re, y_train_re = over_only.fit_resample(\n",
    "                    X_train, y_train)\n",
    "                print('After sampling')\n",
    "                print(Counter(y_train_re['TARGET']))\n",
    "                curr_clf = pipeline.fit(X_train_re, y_train_re)\n",
    "            else:\n",
    "                curr_clf = pipeline.fit(X_train, y_train)\n",
    "\n",
    "            duration = time()-start\n",
    "    \n",
    "            print(clf_name + ' -- fold n°' + str(i))\n",
    "            print('-------------------------------')\n",
    "            scores_df, table_scores = evaluate_and_log(\n",
    "                clf_name, curr_clf, 'train', duration, X_train, y_train, scores_df, table_scores)\n",
    "            scores_df, table_scores = evaluate_and_log(\n",
    "                clf_name, curr_clf, 'test', duration, X_test, y_test, scores_df, table_scores)\n",
    "  \n",
    "        print('===============================')\n",
    "       \n",
    "    return scores_df, table_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "===============================\n",
      "Logistic Regression -- fold n°0\n",
      "-------------------------------\n",
      "Logistic Regression -- fold n°1\n",
      "-------------------------------\n",
      "Logistic Regression -- fold n°2\n",
      "-------------------------------\n",
      "Logistic Regression -- fold n°3\n",
      "-------------------------------\n",
      "Logistic Regression -- fold n°4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:21<01:04, 21.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "RandomForest\n",
      "===============================\n",
      "RandomForest -- fold n°0\n",
      "-------------------------------\n",
      "RandomForest -- fold n°1\n",
      "-------------------------------\n",
      "RandomForest -- fold n°2\n",
      "-------------------------------\n",
      "RandomForest -- fold n°3\n",
      "-------------------------------\n",
      "RandomForest -- fold n°4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [06:42<07:45, 232.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "XGBoost\n",
      "===============================\n",
      "XGBoost -- fold n°0\n",
      "-------------------------------\n",
      "XGBoost -- fold n°1\n",
      "-------------------------------\n",
      "XGBoost -- fold n°2\n",
      "-------------------------------\n",
      "XGBoost -- fold n°3\n",
      "-------------------------------\n",
      "XGBoost -- fold n°4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [08:41<03:00, 180.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Light GBM\n",
      "===============================\n",
      "Light GBM -- fold n°0\n",
      "-------------------------------\n",
      "Light GBM -- fold n°1\n",
      "-------------------------------\n",
      "Light GBM -- fold n°2\n",
      "-------------------------------\n",
      "Light GBM -- fold n°3\n",
      "-------------------------------\n",
      "Light GBM -- fold n°4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [09:09<00:00, 137.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df, table_scores = train_and_evaluate(X_sample, y_sample, with_smote=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>roc AUC score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F2-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model name</th>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Light GBM</th>\n",
       "      <th>test</th>\n",
       "      <td>18.621716</td>\n",
       "      <td>0.756476</td>\n",
       "      <td>0.718481</td>\n",
       "      <td>0.418557</td>\n",
       "      <td>0.172017</td>\n",
       "      <td>0.652271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>18.621716</td>\n",
       "      <td>0.841155</td>\n",
       "      <td>0.738269</td>\n",
       "      <td>0.502148</td>\n",
       "      <td>0.205840</td>\n",
       "      <td>0.784463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Logistic Regression</th>\n",
       "      <th>test</th>\n",
       "      <td>16.593978</td>\n",
       "      <td>0.749323</td>\n",
       "      <td>0.689005</td>\n",
       "      <td>0.416249</td>\n",
       "      <td>0.162145</td>\n",
       "      <td>0.684418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>16.593978</td>\n",
       "      <td>0.752444</td>\n",
       "      <td>0.690007</td>\n",
       "      <td>0.418828</td>\n",
       "      <td>0.163230</td>\n",
       "      <td>0.688265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">RandomForest</th>\n",
       "      <th>test</th>\n",
       "      <td>327.466592</td>\n",
       "      <td>0.719735</td>\n",
       "      <td>0.919300</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>0.509945</td>\n",
       "      <td>0.002014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>327.466592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.999549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">XGBoost</th>\n",
       "      <th>test</th>\n",
       "      <td>110.645997</td>\n",
       "      <td>0.731977</td>\n",
       "      <td>0.768177</td>\n",
       "      <td>0.389792</td>\n",
       "      <td>0.183456</td>\n",
       "      <td>0.542298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>110.645997</td>\n",
       "      <td>0.920286</td>\n",
       "      <td>0.817286</td>\n",
       "      <td>0.623469</td>\n",
       "      <td>0.290292</td>\n",
       "      <td>0.874356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 time  roc AUC score  accuracy  F2-score  \\\n",
       "model name          step                                                   \n",
       "Light GBM           test    18.621716       0.756476  0.718481  0.418557   \n",
       "                    train   18.621716       0.841155  0.738269  0.502148   \n",
       "Logistic Regression test    16.593978       0.749323  0.689005  0.416249   \n",
       "                    train   16.593978       0.752444  0.690007  0.418828   \n",
       "RandomForest        test   327.466592       0.719735  0.919300  0.002515   \n",
       "                    train  327.466592       1.000000  0.999954  0.999549   \n",
       "XGBoost             test   110.645997       0.731977  0.768177  0.389792   \n",
       "                    train  110.645997       0.920286  0.817286  0.623469   \n",
       "\n",
       "                           precision    recall  \n",
       "model name          step                        \n",
       "Light GBM           test    0.172017  0.652271  \n",
       "                    train   0.205840  0.784463  \n",
       "Logistic Regression test    0.162145  0.684418  \n",
       "                    train   0.163230  0.688265  \n",
       "RandomForest        test    0.509945  0.002014  \n",
       "                    train   1.000000  0.999436  \n",
       "XGBoost             test    0.183456  0.542298  \n",
       "                    train   0.290292  0.874356  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores_df = scores_df.groupby(by=['model name', 'step']).agg(\n",
    "    {'time': sum, 'roc AUC score': 'mean', 'accuracy': 'mean', 'F2-score': 'mean', 'precision': 'mean', 'recall': 'mean'})\n",
    "\n",
    "mean_scores_df.to_pickle('half_data_class_weight_scores_df.pkl')\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>roc AUC score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F2-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model name</th>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Light GBM</th>\n",
       "      <th>test</th>\n",
       "      <td>18.621716</td>\n",
       "      <td>4.813485e-03</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.011191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>18.621716</td>\n",
       "      <td>1.384787e-03</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.003950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Logistic Regression</th>\n",
       "      <th>test</th>\n",
       "      <td>16.593978</td>\n",
       "      <td>4.498444e-03</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.006118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>16.593978</td>\n",
       "      <td>9.290700e-04</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">RandomForest</th>\n",
       "      <th>test</th>\n",
       "      <td>327.466592</td>\n",
       "      <td>8.410017e-03</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.193685</td>\n",
       "      <td>0.001366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>327.466592</td>\n",
       "      <td>5.551115e-17</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">XGBoost</th>\n",
       "      <th>test</th>\n",
       "      <td>110.645997</td>\n",
       "      <td>6.399186e-03</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>0.012255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>110.645997</td>\n",
       "      <td>2.217879e-03</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.005187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 time  roc AUC score  accuracy  F2-score  \\\n",
       "model name          step                                                   \n",
       "Light GBM           test    18.621716   4.813485e-03  0.001803  0.007131   \n",
       "                    train   18.621716   1.384787e-03  0.002038  0.003228   \n",
       "Logistic Regression test    16.593978   4.498444e-03  0.003600  0.003328   \n",
       "                    train   16.593978   9.290700e-04  0.001414  0.001160   \n",
       "RandomForest        test   327.466592   8.410017e-03  0.000159  0.001705   \n",
       "                    train  327.466592   5.551115e-17  0.000023  0.000232   \n",
       "XGBoost             test   110.645997   6.399186e-03  0.003470  0.008788   \n",
       "                    train  110.645997   2.217879e-03  0.001958  0.004889   \n",
       "\n",
       "                           precision    recall  \n",
       "model name          step                        \n",
       "Light GBM           test    0.002918  0.011191  \n",
       "                    train   0.001782  0.003950  \n",
       "Logistic Regression test    0.001732  0.006118  \n",
       "                    train   0.000758  0.000705  \n",
       "RandomForest        test    0.193685  0.001366  \n",
       "                    train   0.000000  0.000290  \n",
       "XGBoost             test    0.004513  0.012255  \n",
       "                    train   0.003034  0.005187  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_scores_df = scores_df.groupby(by=['model name', 'step']).agg(\n",
    "    {'time': sum, 'roc AUC score': 'std', 'accuracy': 'std', 'F2-score': 'std', 'precision': 'std', 'recall': 'std'})\n",
    "\n",
    "std_scores_df.to_pickle('half_data_class_weight_std_scores_df.pkl')\n",
    "std_scores_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle Random Forest est le plus long à s'exécuter. Ses résultats sur l'apprentissage montre une grosse tendance à l'overfitting avec ses paramètres par défaut. XG boost a aussi une légère tendance à l'overfitting et sa durée d'execution n'est pas négligeable comparé à LGBM. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vais maintenant comparer les résultats obtenus sur l'apprentissage avec l'ensemble complet, pour la régression logistique et Light GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifiers = data[['SK_ID_CURR']]\n",
    "y = data[['TARGET']]\n",
    "X = data.drop(columns=['SK_ID_CURR','TARGET'])\n",
    "features = X.columns\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=100, stratify = y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "                 ('scaler', RobustScaler()),\n",
    "                 ('model', LogisticRegression(class_weight='balanced'))])\n",
    "               \n",
    "default_logreg_clf = pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results for Base Log Reg\n",
      "-------------------------------\n",
      "Roc auc score : 0.7521\n",
      "F2-score : 0.4183\n",
      "Accuracy :0.6896\n",
      "Precision :0.1630\n",
      "Recall : 0.6877\n",
      "Confusion matrix:\n",
      " [[155992  70156]\n",
      " [  6202  13658]]\n",
      "===============================\n",
      "Validation results for Base Log Reg\n",
      "-------------------------------\n",
      "Roc auc score : 0.7532\n",
      "F2-score : 0.4168\n",
      "Accuracy :0.6899\n",
      "Precision :0.1626\n",
      "Recall : 0.6844\n",
      "Confusion matrix:\n",
      " [[39034 17504]\n",
      " [ 1567  3398]]\n"
     ]
    }
   ],
   "source": [
    "print('Training results for Base Log Reg')\n",
    "print('-------------------------------')\n",
    "evaluate_model(default_logreg_clf, X_train, y_train)\n",
    "print('===============================')\n",
    "print('Validation results for Base Log Reg')\n",
    "print('-------------------------------')\n",
    "evaluate_model(default_logreg_clf, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM - Avant optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.38710976837865"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_pos_weight = Counter(y_train['TARGET'])[0]/Counter(y_train['TARGET'])[1]\n",
    "scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "                 ('scaler', RobustScaler()),\n",
    "                 ('model', LGBMClassifier(objective='binary', scale_pos_weight = scale_pos_weight))])\n",
    "\n",
    "default_lgbm_clf = pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results for Base LGBM\n",
      "-------------------------------\n",
      "Roc auc score : 0.8080\n",
      "F2-score : 0.4665\n",
      "Accuracy :0.7148\n",
      "Precision :0.1859\n",
      "Recall : 0.7494\n",
      "Confusion matrix:\n",
      " [[160968  65180]\n",
      " [  4977  14883]]\n",
      "===============================\n",
      "Validation results for Base LGBM\n",
      "-------------------------------\n",
      "Roc auc score : 0.7649\n",
      "F2-score : 0.4272\n",
      "Accuracy :0.7053\n",
      "Precision :0.1704\n",
      "Recall : 0.6852\n",
      "Confusion matrix:\n",
      " [[39979 16559]\n",
      " [ 1563  3402]]\n"
     ]
    }
   ],
   "source": [
    "print('Training results for Base LGBM')\n",
    "print('-------------------------------')\n",
    "evaluate_model(default_lgbm_clf, X_train, y_train)\n",
    "print('===============================')\n",
    "print('Validation results for Base LGBM')\n",
    "print('-------------------------------')\n",
    "evaluate_model(default_lgbm_clf, X_valid, y_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation de LGBM avec GridSearch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class lightgbm.LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=None, importance_type='split', **kwargs)\n",
    "\n",
    "\n",
    " nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV(estimator: Any, param_grid: Any, *, scoring: Any | None = None, n_jobs: Any | None = None, refit: bool = True, cv: Any | None = None, verbose: int = 0, pre_dispatch: str = \"2*n_jobs\", error_score: float = np.nan, return_train_score: bool = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (246008, 100), Validation set shape: (61503, 100)\n",
      "Train counting: Counter({0.0: 226148, 1.0: 19860}), Validation counting: Counter({0.0: 56538, 1.0: 4965})\n",
      "Fitting 5 folds for each of 41 candidates, totalling 205 fits\n",
      "[CV 1/5; 1/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=100\n",
      "[CV 1/5; 1/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=100;, score=(train=0.421, test=0.410) total time=   6.5s\n",
      "[CV 2/5; 1/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=100\n",
      "[CV 2/5; 1/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=100;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 1/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=100\n",
      "[CV 3/5; 1/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=100;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 1/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=100\n",
      "[CV 4/5; 1/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=100;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 1/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=100\n",
      "[CV 5/5; 1/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=100;, score=(train=0.417, test=0.421) total time=   6.1s\n",
      "[CV 1/5; 2/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=10\n",
      "[CV 1/5; 2/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=10;, score=(train=0.421, test=0.410) total time=   6.1s\n",
      "[CV 2/5; 2/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=10\n",
      "[CV 2/5; 2/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=10;, score=(train=0.418, test=0.419) total time=   6.5s\n",
      "[CV 3/5; 2/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=10\n",
      "[CV 3/5; 2/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=10;, score=(train=0.420, test=0.416) total time=   6.0s\n",
      "[CV 4/5; 2/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=10\n",
      "[CV 4/5; 2/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=10;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 2/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=10\n",
      "[CV 5/5; 2/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=10;, score=(train=0.417, test=0.421) total time=   5.9s\n",
      "[CV 1/5; 3/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=1\n",
      "[CV 1/5; 3/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=1;, score=(train=0.421, test=0.410) total time=   6.0s\n",
      "[CV 2/5; 3/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=1\n",
      "[CV 2/5; 3/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=1;, score=(train=0.418, test=0.419) total time=   6.6s\n",
      "[CV 3/5; 3/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=1\n",
      "[CV 3/5; 3/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=1;, score=(train=0.420, test=0.416) total time=   5.8s\n",
      "[CV 4/5; 3/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=1\n",
      "[CV 4/5; 3/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=1;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 3/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=1\n",
      "[CV 5/5; 3/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=1;, score=(train=0.417, test=0.421) total time=   6.0s\n",
      "[CV 1/5; 4/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.1\n",
      "[CV 1/5; 4/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.1;, score=(train=0.421, test=0.410) total time=   5.8s\n",
      "[CV 2/5; 4/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.1\n",
      "[CV 2/5; 4/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.1;, score=(train=0.418, test=0.419) total time=   6.4s\n",
      "[CV 3/5; 4/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.1\n",
      "[CV 3/5; 4/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.1;, score=(train=0.420, test=0.416) total time=   5.8s\n",
      "[CV 4/5; 4/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.1\n",
      "[CV 4/5; 4/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.1;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 4/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.1\n",
      "[CV 5/5; 4/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.1;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 5/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.01\n",
      "[CV 1/5; 5/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.01;, score=(train=0.421, test=0.410) total time=   5.8s\n",
      "[CV 2/5; 5/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.01\n",
      "[CV 2/5; 5/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.01;, score=(train=0.418, test=0.419) total time=   5.9s\n",
      "[CV 3/5; 5/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.01\n",
      "[CV 3/5; 5/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.01;, score=(train=0.420, test=0.416) total time=   5.9s\n",
      "[CV 4/5; 5/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.01\n",
      "[CV 4/5; 5/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.01;, score=(train=0.418, test=0.421) total time=   6.1s\n",
      "[CV 5/5; 5/41] START classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.01\n",
      "[CV 5/5; 5/41] END classifier=LogisticRegression(class_weight='balanced'), classifier__C=0.01;, score=(train=0.417, test=0.421) total time=   5.9s\n",
      "[CV 1/5; 6/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=100\n",
      "[CV 1/5; 6/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=100;, score=(train=0.421, test=0.410) total time=   6.1s\n",
      "[CV 2/5; 6/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=100\n",
      "[CV 2/5; 6/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=100;, score=(train=0.418, test=0.419) total time=   5.9s\n",
      "[CV 3/5; 6/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=100\n",
      "[CV 3/5; 6/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=100;, score=(train=0.420, test=0.416) total time=   6.0s\n",
      "[CV 4/5; 6/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=100\n",
      "[CV 4/5; 6/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=100;, score=(train=0.418, test=0.421) total time=   6.5s\n",
      "[CV 5/5; 6/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=100\n",
      "[CV 5/5; 6/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=100;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 7/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=1000\n",
      "[CV 1/5; 7/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=1000;, score=(train=0.421, test=0.410) total time=   5.9s\n",
      "[CV 2/5; 7/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=1000\n",
      "[CV 2/5; 7/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=1000;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 7/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=1000\n",
      "[CV 3/5; 7/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=1000;, score=(train=0.420, test=0.416) total time=   5.9s\n",
      "[CV 4/5; 7/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=1000\n",
      "[CV 4/5; 7/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=1000;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 7/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=1000\n",
      "[CV 5/5; 7/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=1000;, score=(train=0.417, test=0.421) total time=   6.5s\n",
      "[CV 1/5; 8/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=10000\n",
      "[CV 1/5; 8/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=10000;, score=(train=0.421, test=0.410) total time=   6.0s\n",
      "[CV 2/5; 8/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=10000\n",
      "[CV 2/5; 8/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=10000;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 8/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=10000\n",
      "[CV 3/5; 8/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=10000;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 8/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=10000\n",
      "[CV 4/5; 8/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=10000;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 8/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=10000\n",
      "[CV 5/5; 8/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=8, classifier__n_estimators=10000;, score=(train=0.417, test=0.421) total time=   5.7s\n",
      "[CV 1/5; 9/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=100\n",
      "[CV 1/5; 9/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=100;, score=(train=0.421, test=0.410) total time=   5.8s\n",
      "[CV 2/5; 9/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=100\n",
      "[CV 2/5; 9/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=100;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 9/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=100\n",
      "[CV 3/5; 9/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=100;, score=(train=0.420, test=0.416) total time=   5.8s\n",
      "[CV 4/5; 9/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=100\n",
      "[CV 4/5; 9/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=100;, score=(train=0.418, test=0.421) total time=   6.7s\n",
      "[CV 5/5; 9/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=100\n",
      "[CV 5/5; 9/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=100;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 10/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=1000\n",
      "[CV 1/5; 10/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=1000;, score=(train=0.421, test=0.410) total time=   6.0s\n",
      "[CV 2/5; 10/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=1000\n",
      "[CV 2/5; 10/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=1000;, score=(train=0.418, test=0.419) total time=   6.1s\n",
      "[CV 3/5; 10/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=1000\n",
      "[CV 3/5; 10/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=1000;, score=(train=0.420, test=0.416) total time=   5.8s\n",
      "[CV 4/5; 10/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=1000\n",
      "[CV 4/5; 10/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=1000;, score=(train=0.418, test=0.421) total time=   5.9s\n",
      "[CV 5/5; 10/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=1000\n",
      "[CV 5/5; 10/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=1000;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 11/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=10000\n",
      "[CV 1/5; 11/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=10000;, score=(train=0.421, test=0.410) total time=   5.8s\n",
      "[CV 2/5; 11/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=10000\n",
      "[CV 2/5; 11/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=10000;, score=(train=0.418, test=0.419) total time=   5.9s\n",
      "[CV 3/5; 11/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=10000\n",
      "[CV 3/5; 11/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=10000;, score=(train=0.420, test=0.416) total time=   5.9s\n",
      "[CV 4/5; 11/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=10000\n",
      "[CV 4/5; 11/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=10000;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 11/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=10000\n",
      "[CV 5/5; 11/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=10, classifier__n_estimators=10000;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 12/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=100\n",
      "[CV 1/5; 12/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=100;, score=(train=0.421, test=0.410) total time=   5.7s\n",
      "[CV 2/5; 12/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=100\n",
      "[CV 2/5; 12/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=100;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 12/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=100\n",
      "[CV 3/5; 12/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=100;, score=(train=0.420, test=0.416) total time=   5.8s\n",
      "[CV 4/5; 12/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=100\n",
      "[CV 4/5; 12/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=100;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 12/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=100\n",
      "[CV 5/5; 12/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=100;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 13/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=1000\n",
      "[CV 1/5; 13/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=1000;, score=(train=0.421, test=0.410) total time=   5.8s\n",
      "[CV 2/5; 13/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=1000\n",
      "[CV 2/5; 13/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=1000;, score=(train=0.418, test=0.419) total time=   6.0s\n",
      "[CV 3/5; 13/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=1000\n",
      "[CV 3/5; 13/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=1000;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 13/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=1000\n",
      "[CV 4/5; 13/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=1000;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 13/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=1000\n",
      "[CV 5/5; 13/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=1000;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 14/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=10000\n",
      "[CV 1/5; 14/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=10000;, score=(train=0.421, test=0.410) total time=   5.7s\n",
      "[CV 2/5; 14/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=10000\n",
      "[CV 2/5; 14/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=10000;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 14/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=10000\n",
      "[CV 3/5; 14/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=10000;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 14/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=10000\n",
      "[CV 4/5; 14/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=10000;, score=(train=0.418, test=0.421) total time=   5.9s\n",
      "[CV 5/5; 14/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=10000\n",
      "[CV 5/5; 14/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=12, classifier__n_estimators=10000;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 15/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=100\n",
      "[CV 1/5; 15/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=100;, score=(train=0.421, test=0.410) total time=   6.0s\n",
      "[CV 2/5; 15/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=100\n",
      "[CV 2/5; 15/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=100;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 15/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=100\n",
      "[CV 3/5; 15/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=100;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 15/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=100\n",
      "[CV 4/5; 15/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=100;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 15/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=100\n",
      "[CV 5/5; 15/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=100;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 16/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=1000\n",
      "[CV 1/5; 16/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=1000;, score=(train=0.421, test=0.410) total time=   5.8s\n",
      "[CV 2/5; 16/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=1000\n",
      "[CV 2/5; 16/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=1000;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 16/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=1000\n",
      "[CV 3/5; 16/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=1000;, score=(train=0.420, test=0.416) total time=   5.8s\n",
      "[CV 4/5; 16/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=1000\n",
      "[CV 4/5; 16/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=1000;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 16/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=1000\n",
      "[CV 5/5; 16/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=1000;, score=(train=0.417, test=0.421) total time=   6.0s\n",
      "[CV 1/5; 17/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=10000\n",
      "[CV 1/5; 17/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=10000;, score=(train=0.421, test=0.410) total time=   5.7s\n",
      "[CV 2/5; 17/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=10000\n",
      "[CV 2/5; 17/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=10000;, score=(train=0.418, test=0.419) total time=   5.9s\n",
      "[CV 3/5; 17/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=10000\n",
      "[CV 3/5; 17/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=10000;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 17/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=10000\n",
      "[CV 4/5; 17/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=10000;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 17/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=10000\n",
      "[CV 5/5; 17/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.02, classifier__max_depth=24, classifier__n_estimators=10000;, score=(train=0.417, test=0.421) total time=   6.0s\n",
      "[CV 1/5; 18/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=100\n",
      "[CV 1/5; 18/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=100;, score=(train=0.421, test=0.410) total time=   5.8s\n",
      "[CV 2/5; 18/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=100\n",
      "[CV 2/5; 18/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=100;, score=(train=0.418, test=0.419) total time=   5.9s\n",
      "[CV 3/5; 18/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=100\n",
      "[CV 3/5; 18/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=100;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 18/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=100\n",
      "[CV 4/5; 18/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=100;, score=(train=0.418, test=0.421) total time=   6.3s\n",
      "[CV 5/5; 18/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=100\n",
      "[CV 5/5; 18/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=100;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 19/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=1000\n",
      "[CV 1/5; 19/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=1000;, score=(train=0.421, test=0.410) total time=   5.7s\n",
      "[CV 2/5; 19/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=1000\n",
      "[CV 2/5; 19/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=1000;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 19/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=1000\n",
      "[CV 3/5; 19/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=1000;, score=(train=0.420, test=0.416) total time=   5.9s\n",
      "[CV 4/5; 19/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=1000\n",
      "[CV 4/5; 19/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=1000;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 19/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=1000\n",
      "[CV 5/5; 19/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=1000;, score=(train=0.417, test=0.421) total time=   5.7s\n",
      "[CV 1/5; 20/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=10000\n",
      "[CV 1/5; 20/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=10000;, score=(train=0.421, test=0.410) total time=   5.7s\n",
      "[CV 2/5; 20/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=10000\n",
      "[CV 2/5; 20/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=10000;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 20/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=10000\n",
      "[CV 3/5; 20/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=10000;, score=(train=0.420, test=0.416) total time=   6.0s\n",
      "[CV 4/5; 20/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=10000\n",
      "[CV 4/5; 20/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=10000;, score=(train=0.418, test=0.421) total time=   5.9s\n",
      "[CV 5/5; 20/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=10000\n",
      "[CV 5/5; 20/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=8, classifier__n_estimators=10000;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 21/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=100\n",
      "[CV 1/5; 21/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=100;, score=(train=0.421, test=0.410) total time=   5.7s\n",
      "[CV 2/5; 21/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=100\n",
      "[CV 2/5; 21/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=100;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 21/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=100\n",
      "[CV 3/5; 21/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=100;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 21/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=100\n",
      "[CV 4/5; 21/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=100;, score=(train=0.418, test=0.421) total time=   5.9s\n",
      "[CV 5/5; 21/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=100\n",
      "[CV 5/5; 21/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=100;, score=(train=0.417, test=0.421) total time=   5.8s\n",
      "[CV 1/5; 22/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=1000\n",
      "[CV 1/5; 22/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=1000;, score=(train=0.421, test=0.410) total time=   5.8s\n",
      "[CV 2/5; 22/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=1000\n",
      "[CV 2/5; 22/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=1000;, score=(train=0.418, test=0.419) total time=   6.0s\n",
      "[CV 3/5; 22/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=1000\n",
      "[CV 3/5; 22/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=1000;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 22/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=1000\n",
      "[CV 4/5; 22/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=1000;, score=(train=0.418, test=0.421) total time=   6.3s\n",
      "[CV 5/5; 22/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=1000\n",
      "[CV 5/5; 22/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=1000;, score=(train=0.417, test=0.421) total time=   6.0s\n",
      "[CV 1/5; 23/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=10000\n",
      "[CV 1/5; 23/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=10000;, score=(train=0.421, test=0.410) total time=   5.7s\n",
      "[CV 2/5; 23/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=10000\n",
      "[CV 2/5; 23/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=10000;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 23/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=10000\n",
      "[CV 3/5; 23/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=10000;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 23/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=10000\n",
      "[CV 4/5; 23/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=10000;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 23/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=10000\n",
      "[CV 5/5; 23/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=10000;, score=(train=0.417, test=0.421) total time=   6.0s\n",
      "[CV 1/5; 24/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=100\n",
      "[CV 1/5; 24/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=100;, score=(train=0.421, test=0.410) total time=   5.9s\n",
      "[CV 2/5; 24/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=100\n",
      "[CV 2/5; 24/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=100;, score=(train=0.418, test=0.419) total time=   5.8s\n",
      "[CV 3/5; 24/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=100\n",
      "[CV 3/5; 24/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=100;, score=(train=0.420, test=0.416) total time=   5.7s\n",
      "[CV 4/5; 24/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=100\n",
      "[CV 4/5; 24/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=100;, score=(train=0.418, test=0.421) total time=   6.0s\n",
      "[CV 5/5; 24/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=100\n",
      "[CV 5/5; 24/41] END classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=100;, score=(train=0.417, test=0.421) total time=   5.9s\n",
      "[CV 1/5; 25/41] START classifier=LGBMClassifier(objective='binary', scale_pos_weight=11.38710976837865), classifier__learning_rate=0.05, classifier__max_depth=12, classifier__n_estimators=1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [74], line 33\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m#Grid search\u001b[39;00m\n\u001b[0;32m     25\u001b[0m grid_cv \u001b[39m=\u001b[39m GridSearchCV(classifier_pipe,\n\u001b[0;32m     26\u001b[0m                     classifier_param_grid,\n\u001b[0;32m     27\u001b[0m                     scoring\u001b[39m=\u001b[39m ftwo_scorer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m                     return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     31\u001b[0m                     verbose\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m grid_cv\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n\u001b[0;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBEST SCORE: \u001b[39m\u001b[39m{\u001b[39;00mgrid_cv\u001b[39m.\u001b[39mbest_score_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m final_classifier_1 \u001b[39m=\u001b[39m grid_cv\u001b[39m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\imblearn\\pipeline.py:272\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    271\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 272\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, yt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    273\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1233\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1231\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1233\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[0;32m   1234\u001b[0m     path_func(\n\u001b[0;32m   1235\u001b[0m         X,\n\u001b[0;32m   1236\u001b[0m         y,\n\u001b[0;32m   1237\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[0;32m   1238\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[0;32m   1239\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[0;32m   1240\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[0;32m   1241\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m   1242\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m   1243\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[0;32m   1244\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[0;32m   1245\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m   1246\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m   1247\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1248\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[0;32m   1249\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[0;32m   1250\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[0;32m   1251\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[0;32m   1252\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1253\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[0;32m   1254\u001b[0m     )\n\u001b[0;32m   1255\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[0;32m   1256\u001b[0m )\n\u001b[0;32m   1258\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:436\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    432\u001b[0m l2_reg_strength \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C\n\u001b[0;32m    433\u001b[0m iprint \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m101\u001b[39m][\n\u001b[0;32m    434\u001b[0m     np\u001b[39m.\u001b[39msearchsorted(np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]), verbose)\n\u001b[0;32m    435\u001b[0m ]\n\u001b[1;32m--> 436\u001b[0m opt_res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[0;32m    437\u001b[0m     func,\n\u001b[0;32m    438\u001b[0m     w0,\n\u001b[0;32m    439\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    440\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    441\u001b[0m     args\u001b[39m=\u001b[39;49m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[0;32m    442\u001b[0m     options\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint, \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: tol, \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_iter},\n\u001b[0;32m    443\u001b[0m )\n\u001b[0;32m    444\u001b[0m n_iter_i \u001b[39m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    445\u001b[0m     solver,\n\u001b[0;32m    446\u001b[0m     opt_res,\n\u001b[0;32m    447\u001b[0m     max_iter,\n\u001b[0;32m    448\u001b[0m     extra_warning_msg\u001b[39m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    449\u001b[0m )\n\u001b[0;32m    450\u001b[0m w0, loss \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\scipy\\optimize\\_minimize.py:692\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    689\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    690\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    691\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 692\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    693\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    694\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    695\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    696\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:362\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    356\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    360\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    363\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    364\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m     \u001b[39m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:187\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads)\u001b[0m\n\u001b[0;32m    185\u001b[0m n_features, n_classes \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_loss\u001b[39m.\u001b[39mn_classes\n\u001b[0;32m    186\u001b[0m n_dof \u001b[39m=\u001b[39m n_features \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept)\n\u001b[1;32m--> 187\u001b[0m weights, intercept, raw_prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_w_intercept_raw(coef, X)\n\u001b[0;32m    189\u001b[0m loss, grad_per_sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_loss\u001b[39m.\u001b[39mloss_gradient(\n\u001b[0;32m    190\u001b[0m     y_true\u001b[39m=\u001b[39my,\n\u001b[0;32m    191\u001b[0m     raw_prediction\u001b[39m=\u001b[39mraw_prediction,\n\u001b[0;32m    192\u001b[0m     sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    193\u001b[0m     n_threads\u001b[39m=\u001b[39mn_threads,\n\u001b[0;32m    194\u001b[0m )\n\u001b[0;32m    195\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\Documents\\Data Scientist\\Projet 7\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:99\u001b[0m, in \u001b[0;36mLinearModelLoss._w_intercept_raw\u001b[1;34m(self, coef, X)\u001b[0m\n\u001b[0;32m     97\u001b[0m         intercept \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     98\u001b[0m         weights \u001b[39m=\u001b[39m coef\n\u001b[1;32m---> 99\u001b[0m     raw_prediction \u001b[39m=\u001b[39m X \u001b[39m@\u001b[39;49m weights \u001b[39m+\u001b[39;49m intercept\n\u001b[0;32m    100\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m     \u001b[39m# reshape to (n_classes, n_dof)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m coef\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting LightGBM. Train shape: {}, Validation set shape: {}\".format(\n",
    "        X_train.shape, X_valid.shape))\n",
    "print(\"Train counting: {}, Validation counting: {}\".format(\n",
    "        Counter(y_train['TARGET']), Counter(y_valid['TARGET'])))\n",
    "\n",
    "classifier_pipe = Pipeline(steps=(['scaler', RobustScaler()],\n",
    "                                ['classifier', LogisticRegression(class_weight='balanced')]))\n",
    "\n",
    "\n",
    "classifier_param_grid = [{\n",
    "                      \"classifier\":[LogisticRegression(class_weight='balanced')],\n",
    "                      \"classifier__C\":[100, 10, 1, 0.1, 0.01],\n",
    "                     },\n",
    "\n",
    "                     {\n",
    "                      \"classifier\":[LGBMClassifier(objective='binary', scale_pos_weight=scale_pos_weight)],\n",
    "                      \"classifier__learning_rate\":[0.02, 0.05, 0.1],\n",
    "                      \"classifier__max_depth\": [8, 10, 12, 24],\n",
    "                      \"classifier__n_estimators\":[100, 1000,10000],\n",
    "            \n",
    "                     }]\n",
    "\n",
    "folds = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "#Grid search\n",
    "grid_cv = GridSearchCV(classifier_pipe,\n",
    "                    classifier_param_grid,\n",
    "                    scoring= ftwo_scorer,\n",
    "                    cv=folds,\n",
    "                    n_jobs=1,\n",
    "                    return_train_score=True,\n",
    "                    verbose=10)\n",
    "\n",
    "grid_cv.fit(X_train,y_train)\n",
    "\n",
    "print(f\"BEST SCORE: {grid_cv.best_score_}\")\n",
    "final_classifier_1 = grid_cv.best_estimator_\n",
    "print(f\"VALIDATION_SCORE: {final_classifier_1.score(X_valid,y_valid)}\")\n",
    "print(f\"\\n\\nBEST CLASSIFIER: {final_classifier_1}\")\n",
    "\n",
    "#print(res.cv_results_)\n",
    "\n",
    "#filename = 'cv_results.sav'\n",
    "#pickle.dump(res.cv_results_, open(filename, 'wb'))\n",
    "\n",
    "print(grid_cv.best_params_)\n",
    "    # model can be saved, used for predictions or scoring\n",
    "best_model = grid_cv.best_estimator_\n",
    "\n",
    "filename = 'final_model_2.sav'\n",
    "pickle.dump(best_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results for best model\n",
      "-------------------------------\n",
      "Roc auc score : 0.7525\n",
      "F2-score : 0.4187\n",
      "Accuracy :0.6897\n",
      "Precision :0.1631\n",
      "Recall : 0.6885\n",
      "Confusion matrix:\n",
      " [[155990  70158]\n",
      " [  6186  13674]]\n",
      "===============================\n",
      "Validation results for best model\n",
      "-------------------------------\n",
      "Roc auc score : 0.7516\n",
      "F2-score : 0.4150\n",
      "Accuracy :0.6897\n",
      "Precision :0.1619\n",
      "Recall : 0.6812\n",
      "Confusion matrix:\n",
      " [[39036 17502]\n",
      " [ 1583  3382]]\n"
     ]
    }
   ],
   "source": [
    "print('Training results for best model')\n",
    "print('-------------------------------')\n",
    "evaluate_model(best_model, X_train, y_train)\n",
    "print('===============================')\n",
    "print('Validation results for best model')\n",
    "print('-------------------------------')\n",
    "evaluate_model(best_model, X_valid, y_valid)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche du seuil de probabilité permettant de maximiser le gain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions de calculs du gain et de représentation graphique de la courbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    " return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "# à maximiser\n",
    "def gain(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    # Par exemple\n",
    "    gain =  2* tn - 10*fn\n",
    "    return gain\n",
    "\n",
    "def plot_gain_scores(threshold_array, gain_scores, precision_scores, recall_scores) :\n",
    "    # Create figure with secondary y-axis\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=threshold_array, y=precision_scores, name=\"precision\"),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=threshold_array, y=recall_scores, name=\"recall\"),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=threshold_array, y=gain_scores, name=\"gain\"),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Gain versus précision/recall\"\n",
    "    )\n",
    "\n",
    "    # Set x-axis title\n",
    "    fig.update_xaxes(title_text=\"Seuil de probabilité\")\n",
    "\n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=\"scores\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"gain\", secondary_y=True)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = best_model.predict_proba(X)[::,1]\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "threshold_array = np.linspace(0, 1, 100)\n",
    "\n",
    "gain_scores = [gain(y, to_labels(y_pred_proba, t)) for t in threshold_array]\n",
    "precision_scores = [precision_score(y, to_labels(y_pred_proba, t)) for t in threshold_array]\n",
    "recall_scores = [recall_score(y, to_labels(y_pred_proba, t)) for t in threshold_array]\n",
    "accuracy_scores = [accuracy_score(y, to_labels(y_pred_proba, t)) for t in threshold_array]\n",
    "\n",
    "# récupération du meilleur seuil (maximisation du gain)\n",
    "\n",
    "maxgain_ix = argmax(gain_scores)\n",
    "best_threshold = threshold_array[maxgain_ix]\n",
    "max_gain = gain_scores[maxgain_ix]\n",
    "\n",
    "print('Seuil=%.3f, gain maximum=%.5f' % (best_threshold, max_gain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gain_scores(threshold_array, gain_scores, precision_scores, recall_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On recalcule les scores avec le nouveau seuil de probabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_with_threshold(best_model,X_train, y_train, best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance globale des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'final_model.sav'\n",
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle Light GBM permet de récupérer l'attribut feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "feature_importance_df['importance'] = model['lgbm'].feature_importances_\n",
    "feature_importance_df.index = features\n",
    "feature_importance_df = feature_importance_df.sort_values(\n",
    "    by='importance', ascending=False)\n",
    "\n",
    "most_important_features = list(feature_importance_df.nlargest(20, columns=['importance']).index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de visualisation des features les plus influentes, à l'échelle globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_global_importance(feature_importance_df, num_features):\n",
    "    df = feature_importance_df.nlargest(num_features, columns=['importance'])\n",
    "    fig = px.bar(df, orientation='h')\n",
    "    fig.update_yaxes(title='Importance')\n",
    "    fig.update_xaxes(title='Feature')\n",
    "    fig.update_traces(showlegend=False)\n",
    "    fig.update_layout(\n",
    "    title=\"Importance globale des features\",\n",
    "    font_size=11,\n",
    "    height=800,\n",
    "    width=600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_global_importance(feature_importance_df, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi visualiser les influences locales respectives sur un sous-ensemble de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample = get_small_sample_for_testing(data,0.01)\n",
    "small_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_small_sample = small_sample[['TARGET']]\n",
    "X_small_sample = small_sample.drop(columns=['SK_ID_CURR','TARGET'])\n",
    "features = X_sample.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light GBM permet grâce à un paramètre (pred_contrib) de calculer les valeurs SHAP de chaque features, par individu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values= model.predict(X_small_sample.values,pred_contrib=True)\n",
    "shap_df = pd.DataFrame(shap_values[:,0:len(features)], columns=features)\n",
    "shap_best_df = shap_df[most_important_features]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des valeurs shap par individu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bee_chart(shap_best_df) :\n",
    "\n",
    "    df = pd.melt(shap_best_df, value_vars=shap_best_df.columns).rename(columns={\n",
    "        \"variable\": \"features\",\n",
    "        \"value\": \"shap_value\"\n",
    "    })\n",
    "    fig = px.scatter(df, y=\"features\", x=\"shap_value\", color='shap_value')\n",
    "    fig.update_traces(marker_size=3)\n",
    "    fig.update_layout(\n",
    "        title=\"Influences locales des features pour chaque point\",\n",
    "        font_size=11,\n",
    "        height=800,\n",
    "        width=800)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bee_chart(shap_best_df) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "290f74c997349af89a0ac2887adbe9207b00d880f9dc68d43a0e78595c455d8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
